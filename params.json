{
  "name": "3d-hand-tracking",
  "tagline": "组员：张倩、曾光、万凯、李超",
  "body": "# Update 1:create a webpage\r\nThis week, we create a webpage and choose the first project in the list: finger tracking as our future work.\r\n\r\n---------------------------------------\r\n\r\n# Update 2:确定项目方案与环境搭建\r\n利用finger tracking 技术可以实现手势的捕抓，以及3D模型的重建，但另一方面，出于实用性和趣味性的考虑，我们希望能够实现和环境的交互，改变或者是调整环境中的东西，比如环境中有一个魔方，希望能够实现魔方的旋转；更进一步，我们希望可以实现和3D场景的交互，比如说捡起环境中的魔方，或者是将魔方移到别的位置。    \r\n因此，我们使用Leap Motion来实现这一功能。Leap Motion能够感知移动的双手，以一种新的方式与电脑交互，它以1%毫米的精度追踪你的10根手指，150°超宽幅的空间视场，用户可以像在真实世界一样随意在3D的空间移动双手。        \r\nLeap Motion与Unity集成：在[https://developer.leapmotion.com/get-started](https://developer.leapmotion.com/get-started)上下载leap motion的sdk，为了集成Unity，还需要下载Leap Motion Unity Core Assets，具体内容参考[http://www.cocoachina.com/game/20150716/12625.html](http://www.cocoachina.com/game/20150716/12625.html)    \r\n配置好环境后，运行自带demo如图所示：![image](http://i4.buimg.com/5f40f122092ec057.jpg)\r\n\r\n---------------------------------------\r\n\r\n# Update 3:实现碰撞检测与旋转\r\n本周我们做了一个实验来实现物体的碰撞检测与旋转，以及简单的leapmotion交互。\r\n在Core Assets中最核心的资源就是HandController,这个是允许我们和Leap Motion设备交互的主要预制体，它作为锚点将双手渲染到场景中。  \r\n![HandController](http://7xrn7f.com1.z0.glb.clouddn.com/16-6-2/71351476.jpg)  \r\n在hc的脚本中包含了左右手的模型，包括    \r\n    \r\n图像模型    \r\n物理模型    \r\n工具模型    \r\n是否头戴的选项（主要用于协同oculus的开发）    \r\n以及其他一些可供调整的选项     \r\n     \r\n需要注意的一点是，由于Unity是以米为单位，而LeapMotion是以毫米为单位，因此这里我们要把scale参数设的大一些，才能使手的大小正常。    \r\n我们将HandController拖到场景中，将坐标设置为（0，-3，3），摄像机应与HandController在同一区域，将摄像机的坐标为（0，0，-3）。    \r\n接下来我们在场景上放入4个立方体，给其中3个添加Material颜色，分别为红色(3,-1,3),蓝色(0,-1,3),橙色(-3,-1,3),最上方的为无色的立方体(0,1,3)，通过手指选择下方有色的立方体，来给无色立方体上色。   \r\n建好的场景如下图：     \r\n![Scene](http://7xrn7f.com1.z0.glb.clouddn.com/16-6-2/96781415.jpg)\r\n为了实现手指选择功能就需要实现碰撞检测，在Unity中我们可以使用OnTriggerEnter函数来检测碰撞，这个函数输入一个Collider对象，因此要想实现碰撞检测，我们需要给之前放入的Cube设置碰撞器，具体内容参考[物理引擎：碰撞器](http://bbs.9ria.com/thread-223673-1-1.html)，这里我们设置的是Mesh Collider。    \r\n实现了碰撞检测后，我们就可以写一个与立方体交互的脚本    \r\n````C#\r\n    using UnityEngine;\r\n    using System.Collections;\r\n\r\n    public class CubeInteraction : MonoBehaviour {\r\n\t    public Color c;\r\n\t    public static Color selectedColor;\r\n\t    public bool selectable = false;\r\n\t    void OnTriggerEnter(Collider c)\r\n\t    {\r\n\t\t    if (c.gameObject.transform.parent.name.Equals(\"index\"))\r\n\t\t    {\r\n\t\t\t    if (this.selectable)\r\n\t\t\t    {\r\n\t\t\t\t    CubeInteraction.selectedColor = this.c;\r\n\t\t\t\t    this.transform.Rotate(Vector3.up, 33);\r\n\t\t\t\t    return;\r\n\t\t\t    }\r\n\t\t\t    transform.gameObject.GetComponent<Renderer>().material.color = CubeInteraction.selectedColor;\r\n\t\t    }\r\n\t    }\r\n    }   \r\n```` \r\n逻辑的核心在OnTriggerEnter(Collider c) 函数内，我们检查与可选择物体的碰撞对象是不是食指，如果是，我们设置为该颜色。我们设计这么一种交互，当食指与可选择颜色的立方体产生碰撞的后，让它旋转33度，这样我们能更直观的看出交互效果。 然后获得立方体的Renderer组件，使它的颜色变为我们选择的颜色。    \r\n    \r\n演示视频：           \r\n\r\n---------------------------------------\r\n#Update 4:获取手部位置与轮廓\r\n经过与老师讨论后，为了更符合项目要求，我们组最终决定更换实验方案，结合OpenCV来实现HandTracking    \r\n首先是OpenCV的配置，我们使用的版本是2.4.9，这部分资料网上有很多，参考资料：[VS2012+OpenCV2.4.9配置](http://blog.csdn.net/liukun321/article/details/38373277)    \r\n为了定位每一帧中手的位置，这里使用基于颜色来识别的方案。首先在开始采样阶段，在图像中给出7个ROI区域，使用者首先将自己的手覆盖这7个区域，用于程序进行颜色采样。    \r\n````C++\r\nvoid waitForPalmCover(MyImage* m){\r\n    m->cap >> m->src;\r\n\tflip(m->src,m->src,1);\r\n\troi.push_back(My_ROI(Point(m->src.cols/3, m->src.rows/6),Point(m->src.cols/3+square_len,m->src.rows/6+square_len),m->src));\r\n\troi.push_back(My_ROI(Point(m->src.cols/4, m->src.rows/2),Point(m->src.cols/4+square_len,m->src.rows/2+square_len),m->src));\r\n\troi.push_back(My_ROI(Point(m->src.cols/3, m->src.rows/1.5),Point(m->src.cols/3+square_len,m->src.rows/1.5+square_len),m->src));\r\n\troi.push_back(My_ROI(Point(m->src.cols/2, m->src.rows/2),Point(m->src.cols/2+square_len,m->src.rows/2+square_len),m->src));\r\n\troi.push_back(My_ROI(Point(m->src.cols/2.5, m->src.rows/2.5),Point(m->src.cols/2.5+square_len,m->src.rows/2.5+square_len),m->src));\r\n\troi.push_back(My_ROI(Point(m->src.cols/2, m->src.rows/1.5),Point(m->src.cols/2+square_len,m->src.rows/1.5+square_len),m->src));\r\n\troi.push_back(My_ROI(Point(m->src.cols/2.5, m->src.rows/1.8),Point(m->src.cols/2.5+square_len,m->src.rows/1.8+square_len),m->src));\t\r\n\tfor(int i =0;i<50;i++){\r\n    \tm->cap >> m->src;\r\n\t\tflip(m->src,m->src,1);\r\n\t\tfor(int j=0;j<NSAMPLES;j++){\r\n\t\t\troi[j].draw_rectangle(m->src);\r\n\t\t}\r\n\t\t\r\n\t\timshow(\"img1\", m->src);\r\n\t\tout << m->src;\r\n        if(cv::waitKey(30) >= 0) break;\r\n\t}\r\n}    \r\n````    \r\n![WaitForPalmCover](http://7xrn7f.com1.z0.glb.clouddn.com/16-6-18/52872741.jpg)    \r\n得到7个阈值后，每一帧都可以计算出7张二值化的图，将它们叠加后再进行一次中值滤波，就可以得到比较清晰的手部区域的二值图。    \r\n![BinaryHand](http://7xrn7f.com1.z0.glb.clouddn.com/16-6-18/28041229.jpg)    \r\n\r\n---------------------------------------\r\n#Update 5:计算凸包统计手指个数\r\n有了之前得到的二值图后，我们可以用OpenCV的函数findContours()来计算二值图中的所有轮廓，从中取最大的轮廓作为手部轮廓。   \r\n利用轮廓然后使用OpenCV函数convexHull()与convexityDefects()可以计算凸包和凸缺陷，从而得到一系列凸顶点和凸缺陷（凹）顶点和凹的深度（其中包含指尖坐标）    \r\n![ConvexHull](http://7xrn7f.com1.z0.glb.clouddn.com/16-6-18/91429732.jpg)\r\n![ConvexityDefects](http://7xrn7f.com1.z0.glb.clouddn.com/16-6-18/75737218.jpg)    \r\n凸包与手掌之间的部分为凸缺陷，每个凸缺陷区域有四个特征量：起始点（startPoint），结束点(endPoint)，距离凸包最远点(farPoint)，最远点到凸包的距离(depth)。一只手有很多凸缺陷，需要一定的策略判断哪些是手指缝那些是干扰，可利用凹的深度和凸点和凹点的夹角（[0,80））进行阈值判断，夹角利用三点的向量积求得，然后我们就可以统计处手指的个数。    \r\n整个实验的流程图：    \r\n![OverView](http://7xrn7f.com1.z0.glb.clouddn.com/16-6-18/52813213.jpg)    \r\n演示视频：    \r\n\r\n\r\n    ",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}